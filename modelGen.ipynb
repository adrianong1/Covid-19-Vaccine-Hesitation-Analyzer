{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelGen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7Bgfw25H049"
      },
      "source": [
        "1. Set constants, and set up tweet scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAQvmexnHK5X",
        "outputId": "9df8e109-ec55-437d-8ea7-6b0b07f3a18f"
      },
      "source": [
        "import tweepy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from bs4 import BeautifulSoup\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# \"true tweets\", \"second filter\", \"no filter\"\n",
        "MODE = \"no filter\"\n",
        "\n",
        "CONSUMER_KEY = \"*********************\"\n",
        "CONSUMER_SECRET = \"*********************\"\n",
        "ACCESS_TOKEN = \"*********************\"\n",
        "ACCESS_TOKEN_SECRET = \"*********************\"\n",
        "\n",
        "SEARCH_WORDS_LIST = [\"covid vaccine unsafe\", \"covid vaccine already had covid\", \"covid vaccine choice\", \"covid vaccine freedom\",\n",
        "                     \"covid vaccine ineffective\", \"covid vaccine not serious health risk\", \"covid vaccine no access\", \"covid vaccine dangerous\",\n",
        "                     \"covid fake\", \"covid vaccine already infected\", \"covid vaccine unproven\", \"covid vaccine dangerous side effects\", \"covid vaccine natural immunity\"]\n",
        "\n",
        "SECOND_FILTER = [\"vacci\", \"vax\"] # One of these words needs to be in the tweet\n",
        "\n",
        "STOP_WORDS = stopwords.words('english')\n",
        "\n",
        "\n",
        "def initialClean(text):\n",
        "  text = BeautifulSoup(text, \"lxml\").text\n",
        "  \n",
        "  # Get rid of numbers\n",
        "  newText = ''.join([i for i in text if not i.isdigit()])\n",
        "  newText = newText.replace(\"_\", \" \")\n",
        "  \n",
        "  # Tokenize string, and remove punctuation\n",
        "  tokens = simple_preprocess(newText, deacc=True)\n",
        "\n",
        "  # Get rid of stopwords\n",
        "  for word in tokens:\n",
        "    if word in STOP_WORDS:\n",
        "      tokens.remove(word)\n",
        "\n",
        "  #print(\"\\n\\nTokens: \", tokens)\n",
        "  \n",
        "  return tokens"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3_NuUD3JPAU"
      },
      "source": [
        "2. Generate the allTweetsArray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx3t_1UaIJJN",
        "outputId": "fef2724d-9aaa-4f46-8500-5568e755eb30"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "allTweetsArray = []\n",
        "\n",
        "try:\n",
        "  api.verify_credentials()\n",
        "  print(\"Authentication OK\")\n",
        "except:\n",
        "  print(\"Error during authentication\")\n",
        "\n",
        "\n",
        "# -------- Get data from LiveStreamedTweets.csv --------\n",
        "csvDf = pd.read_csv('LiveStreamedTweets.csv', delimiter=',')\n",
        "csvNumpyArray = csvDf['Text'].to_numpy()\n",
        "\n",
        "# ----------------- Attempting text cleaning -----------------\n",
        "\n",
        "# Initialize counter\n",
        "countTweets = 0\n",
        "\n",
        "# How many tweets have the search words\n",
        "trueTweetCounter = 0\n",
        "\n",
        "# Set up the sentiment analyzer\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "while countTweets < len(csvNumpyArray):\n",
        "\n",
        "  # Tweet positive sentiment score has to be equal or below 0.4\n",
        "  score = analyser.polarity_scores(csvNumpyArray[countTweets])\n",
        "  if score['pos'] > 0.4:\n",
        "    countTweets += 1\n",
        "    continue\n",
        "\n",
        "  flag = 0\n",
        "  for sw in SEARCH_WORDS_LIST:\n",
        "    count = 0\n",
        "    sw_words = sw.split()\n",
        "    for word in sw_words:\n",
        "      if word in csvNumpyArray[countTweets]:\n",
        "        count += 1\n",
        "        if count == len(sw_words):\n",
        "          #print(\"\\n\\nSW: \", sw)\n",
        "          #print(\"------------------------------\")\n",
        "          #print(\"This tweet contains all words in a search term.\")\n",
        "          #print(csvNumpyArray[countTweets])\n",
        "          #print(\"Word: \", word)\n",
        "          trueTweetCounter += 1\n",
        "          #print(\"trueTweetCounter: \", trueTweetCounter)\n",
        "          #print(\"------------------------------\")\n",
        "          \n",
        "          if MODE == \"true tweets\":\n",
        "            csvNumpyArray[countTweets] = initialClean(csvNumpyArray[countTweets])\n",
        "            allTweetsArray.append(csvNumpyArray[countTweets])\n",
        "          \n",
        "          flag = 1\n",
        "    if flag == 1:\n",
        "      break\n",
        "\n",
        "  if MODE == \"second filter\":\n",
        "    for word in SECOND_FILTER:\n",
        "      if word in csvNumpyArray[countTweets]:\n",
        "        #print(\"\\n\\n{} in {}\".format(word, csvNumpyArray[countTweets]))\n",
        "        #print(word in csvNumpyArray[countTweets])\n",
        "        csvNumpyArray[countTweets] = initialClean(csvNumpyArray[countTweets])\n",
        "        allTweetsArray.append(csvNumpyArray[countTweets])\n",
        "        break\n",
        "  \n",
        "  elif MODE == \"no filter\":\n",
        "    csvNumpyArray[countTweets] = initialClean(csvNumpyArray[countTweets])\n",
        "    allTweetsArray.append(csvNumpyArray[countTweets])\n",
        "  \n",
        "  countTweets += 1\n",
        "\n",
        "print(\"\\n\\n----------------- allTweetsArray -----------------\")\n",
        "print(allTweetsArray)\n",
        "print(\"\\nLength of allTweetsArray: \", len(allTweetsArray))\n",
        "print(\"\\nOriginal Length: \", len(csvDf))\n",
        "print(\"\\ntrueTweetCounter = \", trueTweetCounter)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Authentication OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV3N0xG3TC3a"
      },
      "source": [
        "3. More in depth text preparation (lemmatization, bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd5-V_GETMnR",
        "outputId": "9a483438-3d06-4c2f-e427-9066fa8d4cc1"
      },
      "source": [
        "import spacy\n",
        "\n",
        "bigram = gensim.models.Phrases(allTweetsArray, min_count=1, threshold=3)\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "# Define functions bigrams and lemmatization\n",
        "def make_bigrams(texts):\n",
        "  return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "  texts_out = []\n",
        "  for sent in texts:\n",
        "    doc = nlp(\" \".join(sent))\n",
        "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "  return texts_out\n",
        "\n",
        "# Form Bigrams\n",
        "bigram_tweets = make_bigrams(allTweetsArray)\n",
        "print(\"\\n\\nbigram_tweets: \", bigram_tweets[:1][0])\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(bigram_tweets)\n",
        "\n",
        "print(\"\\n\\ndata_lemmatized: \", data_lemmatized[:1][0])\n",
        "print(\"\\n\\nallTweetsArray: \", allTweetsArray[:1][0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "bigram_tweets:  ['lizziethelawyer_adriennevose', 'joebiden_unvaccinated', 'already', 'covid', 'my', 'natural_immunity', 'better_your', 'https_co', 'mifsxcdkh']\n",
            "\n",
            "\n",
            "data_lemmatized:  ['already', 'covid', 'mifsxcdkh']\n",
            "\n",
            "\n",
            "allTweetsArray:  ['lizziethelawyer', 'adriennevose', 'joebiden', 'unvaccinated', 'already', 'covid', 'my', 'natural', 'immunity', 'better', 'your', 'https', 'co', 'mifsxcdkh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwhM-tmkUxIT"
      },
      "source": [
        "4. Compute multiple LDA models and get their coherence models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQWjaDDMVOJF",
        "outputId": "a17975c4-8f58-4ca2-cf7b-05e07b9c53a2"
      },
      "source": [
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from pprint import pprint\n",
        "import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Creates an LDA model, and build a coherence model\n",
        "def compute_coherence_values(corpus, dictionary, k, b):\n",
        "  lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                          id2word=dictionary,\n",
        "                                          num_topics=k, \n",
        "                                          random_state=100,\n",
        "                                          chunksize=100,\n",
        "                                          passes=10,\n",
        "                                          eta=b)\n",
        "  \n",
        "  coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "  \n",
        "  return coherence_model_lda.get_coherence()\n",
        "\n",
        "# -------- Find the ideal LDA model with the best parameters --------\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# Topics range\n",
        "min_topics = 7\n",
        "max_topics = 13\n",
        "step_size = 1\n",
        "topics_range = range(min_topics, max_topics, step_size)\n",
        "\n",
        "# Beta parameter\n",
        "beta = list(np.arange(0.01, 0.05, 0.1))\n",
        "beta.append('symmetric')\n",
        "\n",
        "# Validation sets\n",
        "num_of_docs = len(corpus)\n",
        "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
        "               corpus]\n",
        "\n",
        "corpus_title = ['75% Corpus', '100% Corpus']\n",
        "\n",
        "model_results = {'Validation_Set': [],\n",
        "                 'Topics': [],\n",
        "                 'Beta': [],\n",
        "                 'Coherence': []\n",
        "                }\n",
        "\n",
        "# Can take a long time to run\n",
        "if True:\n",
        "  pbar = tqdm.tqdm(total=(len(beta)*len(topics_range)*len(corpus_title)))\n",
        "  \n",
        "  # iterate through validation corpuses\n",
        "  for i in range(len(corpus_sets)):\n",
        "    # iterate through number of topics\n",
        "    for k in topics_range:\n",
        "      # iterare through beta values\n",
        "      for b in beta:\n",
        "        # get the coherence score for the given parameters\n",
        "        cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, k=k, b=b)\n",
        "        # Save the model results\n",
        "        model_results['Validation_Set'].append(corpus_title[i])\n",
        "        model_results['Topics'].append(k)\n",
        "        model_results['Beta'].append(b)\n",
        "        model_results['Coherence'].append(cv)\n",
        "            \n",
        "        pbar.update(1)\n",
        "  \n",
        "  pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
        "  pbar.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/24 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/gensim/models/ldamodel.py:1023: RuntimeWarning: divide by zero encountered in log\n",
            "  diff = np.log(self.expElogbeta)\n",
            "100%|██████████| 24/24 [20:29<00:00, 51.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJsFTZgiXGJk"
      },
      "source": [
        "5. Get the attributes of the best model and train it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tRu6uGmXJbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af609e9-8c55-4de7-afcf-b994c8824d17"
      },
      "source": [
        "# Get the attributes of the best model\n",
        "max_index = model_results['Coherence'].index(max(model_results['Coherence']))\n",
        "\n",
        "best_model_info = {'Validation_Set': model_results['Validation_Set'][max_index],\n",
        "                   'Topics': model_results['Topics'][max_index],\n",
        "                   'Beta': model_results['Beta'][max_index],\n",
        "                   'Coherence': model_results['Coherence'][max_index]\n",
        "                  }\n",
        "\n",
        "print(\"\\n\\nBest Model Info: \", best_model_info)\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_lda_model_bigram = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=best_model_info['Topics'], \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           eta=best_model_info['Beta'])\n",
        "\n",
        "pprint(best_lda_model_bigram.print_topics())\n",
        "doc_lda = best_lda_model_bigram[corpus]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Best Model Info:  {'Validation_Set': '100% Corpus', 'Topics': 8, 'Beta': 'symmetric', 'Coherence': 0.4536343421622446}\n",
            "[(0,\n",
            "  '0.041*\"vaccine\" + 0.020*\"life\" + 0.020*\"covid\" + 0.019*\"worker\" + '\n",
            "  '0.018*\"will\" + 0.016*\"spread\" + 0.015*\"would\" + 0.014*\"dangerous\" + '\n",
            "  '0.014*\"work\" + 0.014*\"mandate\"'),\n",
            " (1,\n",
            "  '0.175*\"covid\" + 0.131*\"proof_natural\" + 0.128*\"coronavirus_had\" + '\n",
            "  '0.128*\"drive_vaccine\" + 0.033*\"incredibly_dangerous\" + 0.032*\"shopper\" + '\n",
            "  '0.031*\"listening_shopprsdrugmart\" + 0.020*\"collins_trucke\" + '\n",
            "  '0.020*\"mkfmgdg\" + 0.020*\"never_mandate\"'),\n",
            " (2,\n",
            "  '0.198*\"vaccine\" + 0.121*\"covid\" + 0.074*\"natural_immunity\" + 0.062*\"choice\" '\n",
            "  '+ 0.054*\"get\" + 0.011*\"can\" + 0.010*\"so_thankful\" + 0.010*\"going_give\" + '\n",
            "  '0.010*\"misinformation\" + 0.010*\"child\"'),\n",
            " (3,\n",
            "  '0.122*\"tissues_include\" + 0.122*\"post_vaccination\" + '\n",
            "  '0.122*\"accumulates_organ\" + 0.122*\"vaccine_gets\" + 0.121*\"txpqeaauz\" + '\n",
            "  '0.029*\"vaccinate\" + 0.010*\"breast_milk\" + 0.010*\"yqptcxdlou\" + 0.010*\"risk\" '\n",
            "  '+ 0.007*\"read\"'),\n",
            " (4,\n",
            "  '0.094*\"vaccine\" + 0.074*\"immunity\" + 0.055*\"covid\" + 0.030*\"people\" + '\n",
            "  '0.029*\"take\" + 0.022*\"get\" + 0.021*\"heart\" + 0.016*\"see\" + 0.016*\"say\" + '\n",
            "  '0.015*\"patient\"'),\n",
            " (5,\n",
            "  '0.073*\"dangerous_deadly\" + 0.072*\"be_focuse\" + 0.072*\"movement_spreade\" + '\n",
            "  '0.072*\"lgrvwkwb\" + 0.072*\"horrifying_despicable\" + 0.026*\"covid\" + '\n",
            "  '0.020*\"their_blue\" + 0.020*\"ineffective_a\" + 0.020*\"roll_a\" + '\n",
            "  '0.020*\"tests_are\"'),\n",
            " (6,\n",
            "  '0.205*\"science\" + 0.036*\"still\" + 0.027*\"know\" + 0.016*\"more\" + '\n",
            "  '0.015*\"stop\" + 0.013*\"pandemic\" + 0.012*\"study\" + 0.011*\"sure\" + '\n",
            "  '0.010*\"end\" + 0.010*\"protection\"'),\n",
            " (7,\n",
            "  '0.039*\"freedom\" + 0.039*\"need\" + 0.023*\"death\" + 0.018*\"back\" + '\n",
            "  '0.018*\"us_ruled\" + 0.018*\"avoided_all\" + 0.017*\"court_cancele\" + '\n",
            "  '0.017*\"universal_vax\" + 0.017*\"good\" + 0.017*\"costs_supreme\"')]\n"
          ]
        }
      ]
    }
  ]
}
